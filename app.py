import streamlit as st
from google import genai
import chromadb
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv(override=True)

# Configure Streamlit page
st.set_page_config(
    page_title="RAG Workshop - Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª",
    page_icon="ğŸ¤–",
    layout="wide"
)

# Add RTL support for Arabic
st.markdown("""
<style>
    /* RTL support for the entire app */
    .main, .block-container {
        direction: rtl;
        text-align: right;
    }

    /* RTL for input fields */
    .stTextInput > div > div > input {
        direction: rtl;
        text-align: right;
    }

    /* RTL for text areas */
    .stTextArea textarea {
        direction: rtl;
        text-align: right;
    }

    /* RTL for markdown and text */
    p, h1, h2, h3, h4, h5, h6, li, span {
        direction: rtl;
        text-align: right;
    }

    /* RTL for info/success/warning boxes */
    .stAlert, .stInfo, .stSuccess, .stWarning {
        direction: rtl;
        text-align: right;
    }

    /* RTL for expanders */
    .streamlit-expanderHeader, .streamlit-expanderContent {
        direction: rtl;
        text-align: right;
    }

    /* RTL for sidebar */
    .css-1d391kg, [data-testid="stSidebar"] {
        direction: rtl;
        text-align: right;
    }

    /* Better font for Arabic */
    * {
        font-family: 'Arial', 'Helvetica', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# Initialize API client
@st.cache_resource
def get_client():
    api_key = os.getenv("GEMINI_API_KEY")
    return genai.Client(api_key=api_key)

# Initialize ChromaDB
@st.cache_resource
def get_collection():
    db = chromadb.PersistentClient(path="./chroma_db")
    collection = db.get_or_create_collection(name="my_documents_collection")
    return collection

client = get_client()
collection = get_collection()

# Title
st.title("ğŸ¤– Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª: Ø¨Ø¯ÙˆÙ† RAG vs Ù…Ø¹ RAG")
st.markdown("---")

# Sidebar with info
with st.sidebar:
    st.header("â„¹ï¸ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª")
    st.write(f"Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­ÙÙˆØ¸Ø©: **{collection.count()}**")
    st.markdown("---")
    st.subheader("ğŸ“š Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…ØªÙˆÙØ±Ø©")
    if collection.count() > 0:
        docs = collection.get()
        for i, doc in enumerate(docs['documents'], 1):
            with st.expander(f"Ù…Ø³ØªÙ†Ø¯ {i}"):
                st.write(doc)

# Main interface
user_query = st.text_input(
    "â“ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ù‡Ù†Ø§:",
    # value="Ù…ØªÙ‰ ØªØ£Ø³Ø³Øª Ø´Ø±ÙƒØ© Ø§Ù„Ù†ÙˆØ± ÙˆÙ…Ù† Ù‡Ùˆ Ø±Ø¦ÙŠØ³Ù‡Ø§ Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠØŸ",
    # placeholder="Ù…Ø«Ø§Ù„: ÙƒÙ… Ø£Ø±Ø¨Ø§Ø­ Ø´Ø±ÙƒØ© Ø§Ù„Ù†ÙˆØ±ØŸ"
)

if st.button("ğŸ” Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©", type="primary"):
    if user_query:
        # Create two columns for comparison
        col1, col2 = st.columns(2)

        with col1:
            st.subheader("âŒ Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø¯ÙˆÙ† RAG")
            st.caption("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¯ÙˆÙ† Ø³ÙŠØ§Ù‚")

            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯..."):
                response_without_rag = client.models.generate_content(
                    model="gemini-2.0-flash-lite",
                    contents=user_query
                )
                st.info(response_without_rag.text)

        with col2:
            st.subheader("âœ… Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø¹ RAG")
            st.caption("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹Ø©")

            with st.spinner("Ø¬Ø§Ø±ÙŠ Ø§Ù„Ø¨Ø­Ø« ÙˆØ§Ù„ØªÙˆÙ„ÙŠØ¯..."):
                # Get query embedding
                result = client.models.embed_content(
                    model="gemini-embedding-001",
                    contents=user_query,
                )
                query_embedding = result.embeddings[0].values

                # Retrieve relevant documents
                relevant_docs = collection.query(
                    query_embeddings=[query_embedding],
                    n_results=3
                )

                # Display retrieved context
                context = "\n".join(relevant_docs['documents'][0])
                with st.expander("ğŸ“„ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø³ØªØ±Ø¬Ø¹"):
                    st.write(context)

                # Generate response with RAG
                prompt = f"""Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ. Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø³ÙŠØ§Ù‚ ÙÙ‚Ø·.
Ø¥Ø°Ø§ Ù„Ù… ØªØªÙ…ÙƒÙ† Ù…Ù† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ù‚Ø¯Ù…ØŒ Ø£Ø¬Ø¨ Ø¨Ù€ "Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚."
Ø§Ù„Ø³ÙŠØ§Ù‚:
---
{context}
---
Ø§Ù„Ø³Ø¤Ø§Ù„: {user_query}
Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"""

                response_with_rag = client.models.generate_content(
                    model="gemini-2.0-flash-lite",
                    contents=prompt
                )
                st.success(response_with_rag.text)

        # Show comparison insights
        st.markdown("---")
        st.subheader("ğŸ’¡ Ø§Ù„ÙØ±Ù‚ Ø¨ÙŠÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨ØªÙŠÙ†")
        st.write("""
        - **Ø¨Ø¯ÙˆÙ† RAG**: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¬ÙŠØ¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø±ÙØªÙ‡ Ø§Ù„Ø¹Ø§Ù…Ø© ÙÙ‚Ø· (Ù‚Ø¯ ØªÙƒÙˆÙ† ØºÙŠØ± Ø¯Ù‚ÙŠÙ‚Ø© Ø£Ùˆ Ù‚Ø¯ÙŠÙ…Ø©)
        - **Ù…Ø¹ RAG**: Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¬ÙŠØ¨ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„ØªÙŠ Ù„Ø¯ÙŠÙƒ (Ø£ÙƒØ«Ø± Ø¯Ù‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚ÙŠØ©)
        """)
    else:
        st.warning("âš ï¸ Ø§Ù„Ø±Ø¬Ø§Ø¡ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¤Ø§Ù„ Ø£ÙˆÙ„Ø§Ù‹")

# Footer
st.markdown("---")
st.caption("RAG Workshop - Retrieval Augmented Generation")
